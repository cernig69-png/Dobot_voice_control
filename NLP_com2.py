# -*- coding: utf-8 -*-
"""
STT ‡πÑ‡∏ó‡∏¢/‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏© ‡πÅ‡∏ö‡∏ö‡πÅ‡∏°‡πà‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô (faster-whisper)
- ‡∏ï‡∏£‡∏ß‡∏à‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ (‡πÑ‡∏ó‡∏¢/‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©/‡∏ú‡∏™‡∏°)
- ‡∏õ‡∏£‡∏±‡∏ö‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ñ‡∏≠‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£
- ‡πÉ‡∏ä‡πâ CUDA ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
"""

import os, sys, time, queue, warnings
from pathlib import Path
import numpy as np
import sounddevice as sd
from scipy.io.wavfile import write as wav_write
warnings.filterwarnings("ignore", category=UserWarning, module="ctranslate2")

# -------- Config --------
SAMPLE_RATE = 16000
CHANNELS = 1
RECORD_SECONDS = 6
OUT_WAV = Path("record.wav")

# ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏•‡∏≤‡∏¢‡∏†‡∏≤‡∏©‡∏≤ (‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ‡∏£‡∏∏‡πà‡∏ô .en ‡∏ñ‡πâ‡∏≤‡∏à‡∏∞‡πÄ‡∏≠‡∏≤‡πÑ‡∏ó‡∏¢‡∏î‡πâ‡∏ß‡∏¢)
MODEL_SIZE = os.getenv("WHISPER_MODEL", "small")  # tiny/base/small/medium/large-v3
USE_CUDA = os.getenv("USE_CUDA", "auto")          # auto/cpu/cuda

# ‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå/‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÄ‡∏â‡∏û‡∏≤‡∏∞ (‡∏≠‡∏≠‡∏õ‡∏ä‡∏±‡∏ô) ‡πÄ‡∏ä‡πà‡∏ô ‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏ö‡∏£‡∏ô‡∏î‡πå ‡∏Ñ‡∏≥‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ
INITIAL_PROMPT = os.getenv("INITIAL_PROMPT", "").strip() or None

# -------- Recorder --------
_q = queue.Queue()

def _audio_cb(indata, frames, time_info, status):
    if status:
        print(status, file=sys.stderr, flush=True)
    _q.put(indata.copy())

def record_to_wav(seconds=RECORD_SECONDS):
    print(f"üéôÔ∏è  ‡∏≠‡∏±‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á {seconds}s ... (‡∏û‡∏π‡∏î‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢)")
    frames = []
    with sd.InputStream(samplerate=SAMPLE_RATE, channels=CHANNELS, dtype="float32",
                        callback=_audio_cb):
        start = time.time()
        while time.time() - start < seconds:
            try:
                frames.append(_q.get(timeout=1))
            except queue.Empty:
                pass
    if not frames:
        print("‚ö†Ô∏è  ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏à‡∏≤‡∏Å‡πÑ‡∏°‡∏Ñ‡πå"); return None
    audio = np.concatenate(frames, axis=0)

    # Normalize ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ñ‡∏•‡∏¥‡∏õ/‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÄ‡∏ö‡∏≤‡πÄ‡∏Å‡∏¥‡∏ô
    peak = float(np.max(np.abs(audio))) if audio.size else 0.0
    audio_i16 = np.int16(audio/peak*32767) if peak > 0 else np.int16(audio)

    wav_write(OUT_WAV, SAMPLE_RATE, audio_i16)
    print(f"‚úÖ  ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å: {OUT_WAV.resolve()}")
    return str(OUT_WAV)

# -------- STT --------
def _pick_device():
    if USE_CUDA.lower() == "cuda":
        return "cuda"
    if USE_CUDA.lower() == "cpu":
        return "cpu"
    # auto
    try:
        import torch
        return "cuda" if torch.cuda.is_available() else "cpu"
    except Exception:
        return "cpu"

def load_model():
    from faster_whisper import WhisperModel
    device = _pick_device()
    compute = "float16" if device == "cuda" else "int8"  # ‡πÄ‡∏£‡πá‡∏ß/‡∏Ñ‡∏∏‡πâ‡∏°‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£
    print(f"üß†  ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•: {MODEL_SIZE} (device={device}, compute={compute})")
    t0 = time.time()
    model = WhisperModel(MODEL_SIZE, device=device, compute_type=compute)
    print(f"‚úÖ  ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ({time.time()-t0:.1f}s)")
    return model

def transcribe_file(model, path):
    print("üîé  ‡∏ñ‡∏≠‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á ...")

    # ‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏Ñ‡∏á:
    kwargs = dict(
        # ‡∏†‡∏≤‡∏©‡∏≤=None ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏™‡∏•‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ)
        language=None,
        task="transcribe",
        vad_filter=True,
        vad_parameters={"min_silence_duration_ms": 400, "speech_pad_ms": 100},
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö beam search + ‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡πà‡∏≤ (‡∏Å‡∏±‡∏ô‡∏´‡∏•‡∏∏‡∏î/‡∏°‡∏±‡πà‡∏ß)
        beam_size=5,
        best_of=5,
        temperature=[0.0, 0.2, 0.5],
        # ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏´‡∏¢‡∏∏‡∏î/‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏Ñ‡∏≥‡∏û‡∏π‡∏î
        no_speech_threshold=0.45,
        log_prob_threshold=-1.0,
        compression_ratio_threshold=2.6,
        condition_on_previous_text=False,  # ‡∏Ñ‡∏•‡∏¥‡∏õ‡∏™‡∏±‡πâ‡∏ô‡∏≠‡∏¥‡∏™‡∏£‡∏∞ ‡πÑ‡∏°‡πà‡∏ú‡∏π‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤
        initial_prompt=INITIAL_PROMPT or None,
    )

    segments, info = model.transcribe(str(path), **kwargs)

    print(f"‚ÑπÔ∏è  Detected language: {info.language} (p={info.language_probability:.2f})")
    texts = []
    for seg in segments:
        print(f"[{seg.start:6.2f} ‚Üí {seg.end:6.2f}] {seg.text}")
        texts.append(seg.text)

    full = "".join(texts).strip()
    print("\nüìÑ  ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏ß‡∏°:")
    print(full)
    return full

# -------- Main --------
if __name__ == "__main__":
    try:
        model = load_model()
        while True:
            input("\n‡∏Å‡∏î Enter ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏±‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á (Ctrl+C ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏≠‡∏Å): ")
            wav = record_to_wav(RECORD_SECONDS)
            if not wav:
                continue
            transcribe_file(model, wav)
    except KeyboardInterrupt:
        print("\nüëã ‡∏≠‡∏≠‡∏Å‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°")
